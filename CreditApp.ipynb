{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "Commercial banks receive many credit card applications. Many of them are rejected for various reasons, such as high loan balances, low income levels, or too many inquiries on a person's credit report, for example. Manually analyzing these applications is tedious, error-prone, and time-consuming (and time is money!). Fortunately, this task can be automated with the power of machine learning, and virtually all commercial banks do this nowadays. In this workbook, we will create an automatic credit card approval predictor using machine learning techniques, just as real banks do.\n",
    "\n",
    "### The Data\n",
    "\n",
    "The data is a small subset of the Credit Approval dataset from the UCI Machine Learning Repository, which shows the credit card applications received by a bank. The last column of the dataset is the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto\n",
    "Los bancos comerciales reciben muchas solicitudes de tarjetas de crédito. Muchos de ellos son rechazados por muchas razones, como saldos de préstamos elevados, niveles de ingresos bajos o demasiadas consultas sobre el informe crediticio de una persona, por ejemplo. Analizar manualmente estas aplicaciones es tedioso, propenso a errores y requiere mucho tiempo (¡y el tiempo es dinero!). Afortunadamente, esta tarea se puede automatizar con el poder del aprendizaje automático y prácticamente todos los bancos comerciales lo hacen hoy en día. En este libro de trabajo, se  creará un predictor automático de aprobación de tarjetas de crédito utilizando técnicas de aprendizaje automático, tal como lo hacen los bancos reales.\n",
    "\n",
    "## Datos\n",
    "\n",
    "Los datos se encuentran en un  pequeño subconjunto del conjunto de datos de Aprobación de tarjetas de crédito del Repositorio de aprendizaje automático de UCI que muestra las solicitudes de tarjetas de crédito que recibe un banco. **La última columna del conjunto de datos es el valor objetivo.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Object\n",
    "Supervised learning techniques are used to automate the credit card approval process for banks.\n",
    "\n",
    "Data is preprocessed and supervised learning techniques are applied to find the best model and parameters for the task. The ultimate goal is to find the accuracy score of the best model as a numerical variable, best_score. The target is an accuracy score of at least 0.75.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "El Objetivo\n",
    "Se utilizan  técnicas de aprendizaje supervisado para automatizar el proceso de aprobación de tarjetas de crédito para bancos.\n",
    "\n",
    "Previamente  se procesan los datos y se aplican técnicas de aprendizaje supervisado para encontrar el mejor modelo y parámetros para el trabajo. \n",
    "El objetivo final es encontrar la puntuación de precisión del mejor modelo como una variable numérica, best_score. se apúnta a una puntuación de precisión de al menos 0,75.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "1. Preprocess the data\n",
    "\n",
    "2. Prepare the data for modeling\n",
    "\n",
    "3. Train the model\n",
    "\n",
    "4. Find the best scoring model\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Estrategia\n",
    "\n",
    "1. Preprocesar los datos\n",
    "\n",
    "2. Preparar los datos para modelar\n",
    "\n",
    "3. Entrenamiento  del modelo\n",
    "\n",
    "4. Encontrar el mejor modelo de puntuación\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>g</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>g</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>g</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>s</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0      1      2  3  4  5  6     7  8  9   10 11   12 13\n",
       "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  g    0  +\n",
       "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  g  560  +\n",
       "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  g  824  +\n",
       "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  g    3  +\n",
       "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  s    0  +"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "cc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \n",
    "cc_apps.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Technical Explanation:\n",
    "It involves transforming raw data into a suitable and clean format for analysis and modeling.\n",
    "\n",
    "### Preprocesamiento de Datos\n",
    "\n",
    "Exlicacion Tecnica:\n",
    "Consiste en transformar datos crudos en un formato adecuado y limpio para su análisis y modelado. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b' 'a' '?']\n",
      "0     12\n",
      "1     12\n",
      "2      0\n",
      "3      6\n",
      "4      6\n",
      "5      9\n",
      "6      9\n",
      "7      0\n",
      "8      0\n",
      "9      0\n",
      "10     0\n",
      "11     0\n",
      "12     0\n",
      "13     0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       678 non-null    object \n",
      " 1   1       678 non-null    object \n",
      " 2   2       690 non-null    float64\n",
      " 3   3       684 non-null    object \n",
      " 4   4       684 non-null    object \n",
      " 5   5       681 non-null    object \n",
      " 6   6       681 non-null    object \n",
      " 7   7       690 non-null    float64\n",
      " 8   8       690 non-null    object \n",
      " 9   9       690 non-null    object \n",
      " 10  10      690 non-null    int64  \n",
      " 11  11      690 non-null    object \n",
      " 12  12      690 non-null    int64  \n",
      " 13  13      690 non-null    object \n",
      "dtypes: float64(2), int64(2), object(10)\n",
      "memory usage: 75.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# para conocer si hay valores faltantes debo buscar valores nulos que no son especificamente NAN en cada columna con unique()\n",
    "# en este caso lei la respuesta la columna 0 tiene valores \"?\"\n",
    "\n",
    "print(cc_apps[0].unique())\n",
    "cc_apps_replaced = cc_apps.replace(\"?\", np.NaN)\n",
    "cc_app_imputed = cc_apps_replaced.copy()\n",
    "print(cc_apps_replaced.isna().sum())\n",
    "print(cc_apps_replaced.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recorra las columnas del dataframe \n",
    "for col in cc_app_imputed.columns:\n",
    "    #si la columna es un objeto\n",
    "    if cc_app_imputed[col].dtype== \"object\":\n",
    "        # impute el valor mas frecuente utilizando filna para encontrar NA \n",
    "        cc_app_imputed[col]= cc_app_imputed[col].fillna(cc_app_imputed[col].value_counts().index[0])\n",
    "    else:\n",
    "        cc_app_imputed[col] = cc_app_imputed[col].fillna(cc_app_imputed[col].mean())\n",
    "#Ahora la columna 0 tiene el valor mas frecuente y la columna que contiene numeros tiene el promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cc_app_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_app_encoded= pd.get_dummies(cc_app_imputed, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cc_app_encoded.iloc[:,:-1].values\n",
    "y= cc_app_encoded.iloc[:,[-1]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Technical Explanation:\n",
    "\n",
    "Logistic regression is a powerful technique for binary classification that allows predicting the probability of an instance belonging to a specific class based on the provided features.\n",
    "\n",
    "\n",
    "### Entrenamiento del Modelo\n",
    "Explicacion Tecnica:\n",
    "\n",
    "La regresión logística es una técnica poderosa para clasificación binaria que permite predecir la probabilidad de una instancia pertenezca a una clase específica basándose en las características proporcionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del conjunto de entrenamiento (X_train): (462, 382)\n",
      "Dimensiones del conjunto de prueba (X_test): (228, 382)\n",
      "Dimensiones del conjunto de entrenamiento (y_train): (462, 1)\n",
      "Dimensiones del conjunto de prueba (y_test): (228, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33,random_state=42)\n",
    "\n",
    "# Mostrar las dimensiones de los conjuntos\n",
    "print(\"Dimensiones del conjunto de entrenamiento (X_train):\", X_train.shape)\n",
    "print(\"Dimensiones del conjunto de prueba (X_test):\", X_test.shape)\n",
    "print(\"Dimensiones del conjunto de entrenamiento (y_train):\", y_train.shape)\n",
    "print(\"Dimensiones del conjunto de prueba (y_test):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento escalado:\n",
      "[[-0.45027582  0.82127658 -0.49002119 ... -0.8814324  -0.12403473\n",
      "  -0.2818497 ]\n",
      " [-0.39786487  0.73765827  0.69733785 ...  1.13451695 -0.12403473\n",
      "  -0.2818497 ]\n",
      " [-0.6599196  -0.60023466 -0.49002119 ... -0.8814324  -0.12403473\n",
      "  -0.2818497 ]\n",
      " [-0.74692177 -0.67047404 -0.49002119 ... -0.8814324  -0.12403473\n",
      "  -0.2818497 ]\n",
      " [ 0.10003911  0.9885132  -0.49002119 ... -0.8814324  -0.12403473\n",
      "  -0.2818497 ]]\n",
      "Conjunto de prueba escalado:\n",
      "[[-0.6599196  -0.68385297 -0.09423485 ...  1.13451695 -0.12403473\n",
      "  -0.2818497 ]\n",
      " [-0.13581015 -0.68385297 -0.49002119 ... -0.8814324  -0.12403473\n",
      "  -0.2818497 ]\n",
      " [-0.97438528 -0.51661635 -0.49002119 ... -0.8814324  -0.12403473\n",
      "  -0.2818497 ]\n",
      " [ 0.38829931 -0.34937973 -0.49002119 ... -0.8814324  -0.12403473\n",
      "  -0.2818497 ]\n",
      " [-0.97438528 -0.68385297 -0.49002119 ... -0.8814324  -0.12403473\n",
      "   3.54799032]]\n"
     ]
    }
   ],
   "source": [
    "scaler =StandardScaler()\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.transform(X_test)\n",
    "\n",
    "# Mostrar las primeras filas del conjunto de entrenamiento escalado\n",
    "print(\"Conjunto de entrenamiento escalado:\")\n",
    "print(rescaledX_train[:5])\n",
    "\n",
    "# Mostrar las primeras filas del conjunto de prueba escalado\n",
    "print(\"Conjunto de prueba escalado:\")\n",
    "print(rescaledX_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmcar\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(rescaledX_train, y_train)\n",
    "y_train_pred= logreg.predict(rescaledX_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is a tool that allows you to evaluate the performance of a classification model.\n",
    "\n",
    "\n",
    "La matriz de confusión es una herramienta que te permite evaluar el rendimiento de un modelo de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[203   1]\n",
      " [  1 257]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix( y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Conclusion:\n",
    "\n",
    "The confusion matrix indicates that the logistic regression model is performing extremely well on the training set, with only 2 errors out of a total of 462 predictions.\n",
    "\n",
    "The score of this model is higher than recommended. In this case, we assume it is incorrect and will look to adjust hyperparameters to find a model that returns the target score of around 75%.\n",
    "\n",
    "### Primera Conclusion :\n",
    "\n",
    "La matriz de confusión indica que el modelo de regresión logística está funcionando extremadamente bien en el conjunto de entrenamiento, con solo 2 errores en un total de 462 predicciones\n",
    "\n",
    "El score de este modelo es mas alto de lo recomndable, en este caso asumimos que es incorrecto y buscaremos ajustar hiperparametros para dar con el modelo que devuelba el score objetivo de alrededor 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Model\n",
    "Hyperparameters\n",
    "\n",
    "Technical Explanation:\n",
    "\n",
    "What does GridSearchCV do?\n",
    "\n",
    "GridSearchCV performs an exhaustive search over a specified set of hyperparameter values. It will test all possible combinations of hyperparameters and evaluate the model's performance for each combination using cross-validation. In the end, it tells you which combination of hyperparameters is the best based on the chosen evaluation metric.\n",
    "\n",
    "\n",
    "### Encontrar el Mejor modelo \n",
    "\n",
    "Hiperparametros\n",
    "\n",
    "Explicacion Tecnica:\n",
    "\n",
    "¿Qué hace GridSearchCV?\n",
    "\n",
    "GridSearchCV realiza una búsqueda exhaustiva sobre un conjunto especificado de valores de hiperparámetros. Probará todas las combinaciones posibles de los hiperparámetros y evaluará el rendimiento del modelo para cada combinación usando validación cruzada. Al final, te dice cuál es la mejor combinación de hiperparámetros basada en la métrica de evaluación que se haya elegido \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol =[0.01,0.001,0.00001]\n",
    "max_iter=[100,150,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid =dict(tol=tol, max_iter=max_iter)\n",
    "grid_model = GridSearchCV(estimator=logreg,param_grid=param_grid,cv=5)\n",
    "grid_model_result = grid_model.fit(rescaledX_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.818256 using {'max_iter': 100, 'tol': 0.01}\n"
     ]
    }
   ],
   "source": [
    "best_train_score, best_train_params = grid_model_result.best_score_,grid_model_result.best_params_\n",
    "\n",
    "print(\"Best: %f using %s\" % (best_train_score,best_train_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier:  0.7982456140350878\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_model_result.best_estimator_\n",
    "best_score = best_model.score(rescaledX_test, y_test)\n",
    "\n",
    "print(\"Accuracy of logistic regression classifier: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion:\n",
    "A project is carried out using data preprocessing strategies to create an ML model based on logistic regression. The initial result is evaluated, and it is concluded that hyperparameters need to be adjusted to select the best possible fit. The final result aligns with the desired objective.\n",
    "\n",
    "Going forward, it remains to test other models and adjustments to determine continuous improvement in the model's results.\n",
    "\n",
    "## Conclucion Final:\n",
    "\n",
    "Se realiza un proyecto utlizando estrategias de preprosesamiento de datos, para poder realizar un modelo de ML basado en regression logistica. El primer resultado es evaludao y se concluye que se debe ajustar hipermparametros para seleccionar el mejor ajuste posible, El resultado final consuerda con el objetivo buscado.\n",
    "\n",
    "En adelante resta probar otros modelos y otros ajustes para determinar una mejora continua en los resultados del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuentes:\n",
    "\n",
    "* https://projects.datacamp.com/projects/1908\n",
    "\n",
    "* https://www.datacamp.com/tutorial/preprocessing-in-data-science-part-1-centering-scaling-and-knn\n",
    "\n",
    "* https://www.datacamp.com/tutorial/preprocessing-in-data-science-part-2-centering-scaling-and-logistic-regression\n",
    "\n",
    "* https://www.datacamp.com/tutorial/preprocessing-in-data-science-part-3-scaling-synthesized-data\n",
    "\n",
    "* https://www.datacamp.com/tutorial/categorical-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
